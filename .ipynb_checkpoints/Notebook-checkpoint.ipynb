{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visual results (pixel-wise class predictions and model uncertainty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, the main results of our best model are recreated. We show the prediction of the class labels and the model uncertainty for different images in the train, validation and test datasets, as well as for other images. In order to do so, we use the method *visual_results* from the *SegNet* class which is imported in the first code line of this notebook. The \"visual_results\" method can take three arguments: \n",
    "1. ***dataset_type***: A string indicating the dataset type (either \"TRAIN\", \"VAL\" or \"TEST\"). By default it is \"TEST\".\n",
    "2. ***images_index***: A list indicating the images indexes from the corresponding dataset. For instance, if we pass the list [0,20,47] we are requesting the prediction for the images that are found in the indexes 0, 20 and 47 of the dataset. This parameter can also be an integer value which corresponds to the number of different images that we want to display. In this case, we randomly pick the specified number of images from the corresponding dataset. By default it is set to 3.\n",
    "3. ***FLAG_MAX_VOTE***: A boolean parameter indicating whether we want to use Max Voting (True) or Mean (False) at test time for the Bayesian model. By default it is set to False.\n",
    "\n",
    "\n",
    "For each of the displayed images we show (1) the original image, (2) the ground truth, i.e., the given pixel labels for training the network and obtain prediction accuracies, (3) the output of our model for the given image, and (4) the uncertainty of the model prediction (the darker the color is, the more uncertain the prediction in this pixel is).\n",
    "\n",
    "Finally, we also show the color legend for the different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to run this notebook you need to have the files from https://github.com/toimcio/SegNet-tensorflow in the same directory than this notebook. Moreover, you need the 'vgg16.npy' file also in the running directory, which can be found in this link: https://mega.nz/#!YU1FWJrA!O1ywiCS2IiOlUCtCpI6HTJOMrneN-Qdv3ywQP5poecM. Finally, you need to download the saved model from https://drive.google.com/drive/folders/1tD-4FsN6s8c45R81bjqIG_f5VCS1j-Db and put the downloaded folder also in the running directory. Doing so the notebook should run without any problem. If there is any problem, make sure that the file and directory paths fit the ones from the 'config.json' file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from SegNet import SegNet\n",
    "from drawings_object import display_color_legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train dataset (367 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegNet().visual_results(dataset_type = \"TRAIN\", images_index = 2, FLAG_MAX_VOTE = False)\n",
    "display_color_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation dataset (101 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SegNet().visual_results(\"VAL\", [0,50], False)\n",
    "display_color_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dataset (233 images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index of the image with the worst global accuracy: 62\n",
    "# Index of the image with an average global accuracy: 66\n",
    "# Index of the image with the best global accuracy: 198\n",
    "SegNet().visual_results(\"TEST\", [62,66,198], False)\n",
    "display_color_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## External images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also show the results for external images, i.e., images that are not from any dataset. In order to do so, we use the method *visual_results_external_image* from the *SegNet* class. This method can take two arguments:\n",
    "1. ***images***: a list with the different images that we want to show in numpy.ndarray format. Each of the different images must have a shape (X, X, 3), i.e., with 3 channels.\n",
    "2. ***FLAG_MAX_VOTE***: A boolean parameter indicating whether we want to use Max Voting (True) or Mean (False) at test time for the Bayesian model. By default it is set to False.\n",
    "\n",
    "As an example, we take screenshots in Google Maps (in Street View mode) of some streets in the DTU Lyngby Campus. In this case, we show the original image, the labeled output and the prediction uncertainty (there is no ground truth for obvious reasons). In this case, we also calculate the inference time for each of the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import misc\n",
    "\n",
    "images = [misc.imread('./external_images/DTU_Campus_Image1.jpg'), \n",
    "          misc.imread('./external_images/DTU_Campus_Image2.jpg'), \n",
    "          misc.imread('./external_images/DTU_Campus_Image3.jpg')]\n",
    "\n",
    "pred_tot, var_tot, inference_time = SegNet().visual_results_external_image(images)\n",
    "\n",
    "display_color_legend()\n",
    "\n",
    "for t in range(len(inference_time)):\n",
    "    print(\"Inference time for Image %d: %.2f seconds\" % (t+1, inference_time[t]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the inference time is of about 27 seconds, which corresponds to less than 1 second for each of the 30 stochastic dropout samples that we generate at test time in the Bayesian SegNet model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
